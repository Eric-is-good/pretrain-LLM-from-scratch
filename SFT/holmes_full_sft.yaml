### æ­£å¼è®­ç»ƒå‰æ¸…ç† token ç¼“å­˜
### model
model_name_or_path: /home/users/nus/e1352533/scratch/training/sft_base  # ä½ çš„holmesæ¨¡å‹è·¯å¾„
trust_remote_code: true  # é‡è¦ï¼šå…è®¸åŠ è½½è‡ªå®šä¹‰ä»£ç 
# resume_from_checkpoint: /home/users/nus/e1352533/scratch/training/test/checkpoint-20 # ğŸ‘ˆ å¤åˆ¶ä¸€ä¸ªåœ°æ–¹


### method
stage: sft
do_train: true
finetuning_type: full  # å…¨é‡å¾®è°ƒ
flash_attn: fa2  # å¯ç”¨FlashAttention-2
enable_liger_kernel: true
gradient_checkpointing: true
optim: adamw_bnb_8bit  # ä½¿ç”¨8-bit AdamW
# optim: adamw_torch_fused
deepspeed: examples/deepspeed/ds_z2_config.json  # å¯é€‰ï¼šä½¿ç”¨DeepSpeed 

### dataset
dataset: jiangshu_think,nv_sft_trans,ring_math_trans,sharegpt_1,sharegpt_2,en_zh_mix,zh_en_mix,baike,bloss_chat,deepseek,zhihu,openmathinstruct_trans,openmathinstruct2_sample,jiangshu
template: qwen3  # ä½¿ç”¨qwen3æ¨¡æ¿
packing: true
cutoff_len: 4096
overwrite_cache: false # ğŸ‘ˆ åç»­æ”¹ä¸º false
cache_dir: /home/users/nus/e1352533/scratch/hf_cache  # æŒ‡å®šHFç¼“å­˜ä½ç½®
preprocessing_num_workers: 16
dataloader_num_workers: 4
tokenized_path: /home/users/nus/e1352533/scratch/hf_cache/sft_token  # ğŸ‘ˆ token ä¿å­˜è·¯å¾„

### template settings (å…³é”®éƒ¨åˆ†)
enable_thinking: null  # è®¾ç½®ä¸ºnullè¡¨ç¤ºæ··åˆæ¨¡å¼ (True=å¯ç”¨æ€ç»´é“¾, False=ç¦ç”¨, null=æ··åˆ)

### output
output_dir: /home/users/nus/e1352533/scratch/training/ckp/
logging_steps: 1
save_steps: 500
save_total_limit: 2 
plot_loss: true
overwrite_output_dir: true
save_only_model: false
report_to: none  # å¯é€‰: wandb, tensorboard, swanlab

### train
per_device_train_batch_size: 4
gradient_accumulation_steps: 16
learning_rate: 6.0e-5
num_train_epochs: 1.0
lr_scheduler_type: cosine_with_min_lr  # å¸¦æœ€å°å­¦ä¹ ç‡çš„ä½™å¼¦
warmup_ratio: 0.01
lr_scheduler_kwargs: 
  min_lr: 6.0e-6     
bf16: true
ddp_timeout: 180000000

### eval (å¯é€‰)
# eval_dataset: alpaca_en_demo
# val_size: 0.1
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 500